{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tr\n",
    "from torch import nn\n",
    "\n",
    "### CONVOLUTION\n",
    "\n",
    "\n",
    "# Definición de N_Conv: Una secuencia de Conv1d -> BatchNorm1d -> ReLU\n",
    "class N_Conv(nn.Module):\n",
    "    \"\"\"([Conv] -> [BatchNorm] -> [ReLu]) x N\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels,\n",
    "        output_channels,\n",
    "        num_conv,\n",
    "        kernel_size=3,\n",
    "        padding=1,\n",
    "        stride=1,\n",
    "        AVG_POOL=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_conv):\n",
    "            if AVG_POOL:\n",
    "                layers.append(nn.AvgPool1d(kernel_size=2, stride=2, padding=0))\n",
    "\n",
    "            if i != 0:\n",
    "                layers.append(\n",
    "                    nn.Conv1d(\n",
    "                        output_channels,\n",
    "                        output_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=padding,\n",
    "                        stride=stride,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                layers.append(\n",
    "                    nn.Conv1d(\n",
    "                        input_channels,\n",
    "                        output_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=padding,\n",
    "                        stride=stride,\n",
    "                    )\n",
    "                )\n",
    "            layers.append(nn.BatchNorm1d(output_channels))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        self.N_Conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.N_Conv(x)\n",
    "\n",
    "\n",
    "# Definición de N_Conv: Una secuencia de Conv1d -> BatchNorm1d -> ReLU\n",
    "\n",
    "\n",
    "class Max_Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then N_Conv\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, num_conv, kernel_size=3, padding=1, stride=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2, padding=0),\n",
    "            N_Conv(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                num_conv,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                stride=stride,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Avg_Down(nn.Module):\n",
    "    \"\"\"Downscaling with avgpool then N_Conv\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, num_conv, kernel_size=3, padding=1, stride=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.avgpool_conv = nn.Sequential(\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2, padding=0),\n",
    "            N_Conv(in_channels, out_channels, num_conv, kernel_size, padding, stride),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.avgpool_conv(x)\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloque de upsampling con conexión skip opcional y fusión (concatenación o suma),\n",
    "    seguido de una secuencia de convoluciones (N_Conv).\n",
    "\n",
    "    Args:\n",
    "        in_channels: Canales de entrada para el upsampling.\n",
    "        out_channels: Canales de salida deseados.\n",
    "        num_conv: Número de capas en N_Conv.\n",
    "        up_mode: Método de upsampling: \"upsample\" o \"transpose\".\n",
    "        addition: Modo de fusión en la conexión skip: \"cat\" (concatenar) o \"sum\" (sumar).\n",
    "        skip: Si True se utiliza la conexión skip con la entrada x2.\n",
    "        kernel_size: Tamaño del kernel para N_Conv (por defecto 3).\n",
    "        padding: Padding para N_Conv (por defecto 1).\n",
    "        stride: Stride para N_Conv (por defecto 1).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        num_conv: int,\n",
    "        up_mode: str = \"upsample\",\n",
    "        skip: bool = True,\n",
    "        addition: str = \"cat\",\n",
    "        kernel_size: int = 3,\n",
    "        padding: int = 1,\n",
    "        stride: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.skip = skip\n",
    "        self.addition = addition\n",
    "        self.up_mode = up_mode\n",
    "\n",
    "        if up_mode not in [\"upsample\", \"transpose\"]:\n",
    "            raise ValueError(\n",
    "                'El parámetro \"up_mode\" debe ser \"upsample\" o \"transpose\".'\n",
    "            )\n",
    "\n",
    "        # Configuración del upsampling y determinación de canales tras el up.\n",
    "        if up_mode == \"upsample\":\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"linear\", align_corners=True)\n",
    "            up_out_channels = in_channels  # canales permanecen iguales en upsample\n",
    "        else:  # \"transpose\"\n",
    "            self.up = nn.ConvTranspose1d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                output_padding=1,\n",
    "            )\n",
    "            up_out_channels = out_channels\n",
    "\n",
    "        # Definir los canales de entrada para la siguiente convolución.\n",
    "        if skip:\n",
    "            if addition == \"cat\":\n",
    "                conv_in_channels = up_out_channels + out_channels\n",
    "            elif addition == \"sum\":\n",
    "                conv_in_channels = out_channels\n",
    "                if up_mode == \"upsample\":\n",
    "                    self.adjust = nn.Conv1d(\n",
    "                        up_out_channels, out_channels, kernel_size=1\n",
    "                    )\n",
    "            else:\n",
    "                raise ValueError('El parámetro \"addition\" debe ser \"cat\" o \"sum\".')\n",
    "        else:\n",
    "            conv_in_channels = up_out_channels\n",
    "\n",
    "        self.conv = N_Conv(\n",
    "            conv_in_channels, out_channels, num_conv, kernel_size, padding, stride\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        if self.skip:\n",
    "            if x2 is None:\n",
    "                raise ValueError(\"Se requiere x2 para la conexión skip.\")\n",
    "            if self.addition == \"cat\":\n",
    "                diff = x2.size(2) - x1.size(2)\n",
    "                x1 = nn.functional.pad(x1, [diff // 2, diff - diff // 2])\n",
    "                x = torch.cat([x2, x1], dim=1)\n",
    "            elif self.addition == \"sum\":\n",
    "                if self.up_mode == \"upsample\":\n",
    "                    x1 = self.adjust(x1)\n",
    "                x = x2 + x1\n",
    "        else:\n",
    "            x = x1\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# Definición de un modelo simple similar a U-Net para pruebas\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=4,\n",
    "        num_conv=2,\n",
    "        features=[4, 8, 8],\n",
    "        up_mode=\"upsample\",\n",
    "        addition=\"sum\",\n",
    "        skip=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa inicial: de embedding_dim a 4 canales\n",
    "        self.inc = N_Conv(embedding_dim, features[0], num_conv)\n",
    "        # Dos bloques de downsampling\n",
    "        self.down1 = Max_Down(features[0], features[1], num_conv)  # 128 -> 64\n",
    "        self.down2 = Max_Down(features[1], features[2], num_conv)  # 64 -> 32\n",
    "        # Dos bloques de upsampling\n",
    "        # Primer bloque: combina salida de down2 (8 canales) con down1 (8 canales) -> in_channels=16, produce 8\n",
    "        self.up1 = UpBlock(\n",
    "            features[2],\n",
    "            features[1],\n",
    "            num_conv,\n",
    "            up_mode=up_mode,\n",
    "            addition=addition,\n",
    "            skip=skip,\n",
    "        )\n",
    "        # Segundo bloque: combina salida de up1 (8 canales) con inc (4 canales) -> in_channels=12, produce 4\n",
    "        self.up2 = UpBlock(\n",
    "            features[1],\n",
    "            features[0],\n",
    "            num_conv,\n",
    "            up_mode=up_mode,\n",
    "            addition=addition,\n",
    "            skip=skip,\n",
    "        )\n",
    "        # Capa final: reduce a la dimensión original de embedding (por ejemplo, 4)\n",
    "        self.outc = nn.Conv1d(features[0], embedding_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x de tamaño [batch, embedding_dim, L], con L=128 en nuestro ejemplo\n",
    "        print(\"Input shape:\", x.shape)\n",
    "        x1 = self.inc(x)\n",
    "        print(\"After inc:\", x1.shape)\n",
    "        x2 = self.down1(x1)\n",
    "        print(\"After down1:\", x2.shape)\n",
    "        x3 = self.down2(x2)\n",
    "        print(\"After down2 (latente):\", x3.shape)\n",
    "        # Decodificación: primero upsample y concatena con la salida de down1\n",
    "        x_up1 = self.up1(x3, x2)\n",
    "        print(\"After up1:\", x_up1.shape)\n",
    "        # Luego upsample y concatena con la salida inicial\n",
    "        x_up2 = self.up2(x_up1, x1)\n",
    "        print(\"After up2:\", x_up2.shape)\n",
    "        out = self.outc(x_up2)\n",
    "        print(\"Output shape:\", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m upsample a sum s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a sum s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a cat s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a cat s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a sum s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a sum s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a cat s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a cat s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a sum s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a sum s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a cat s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a cat s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a sum s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a sum s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a cat s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a cat s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a sum s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a sum s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a cat s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a cat s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a sum s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a sum s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a cat s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a cat s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a sum s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a sum s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a cat s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a cat s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a sum s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a sum s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a cat s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a cat s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a sum s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a sum s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a cat s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a cat s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a sum s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a sum s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a cat s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a cat s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a sum s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a sum s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a cat s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m upsample a cat s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a sum s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a sum s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a cat s True\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n",
      "m transpose a cat s False\n",
      "Input shape: torch.Size([4, 4, 128])\n",
      "After inc: torch.Size([4, 4, 128])\n",
      "After down1: torch.Size([4, 8, 64])\n",
      "After down2 (latente): torch.Size([4, 8, 32])\n",
      "After up1: torch.Size([4, 8, 64])\n",
      "After up2: torch.Size([4, 4, 128])\n",
      "Output shape: torch.Size([4, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "# Prueba: Creamos un batch de ejemplo y ejecutamos el modelo\n",
    "batch_size = 4\n",
    "embedding_dim = 4\n",
    "num_conv = (2,)\n",
    "features = ([4, 8, 8],)\n",
    "up_mode = (\"upsample\",)\n",
    "addition = (\"sum\",)\n",
    "skip = (True,)\n",
    "L = 128  # Longitud temporal inicial\n",
    "\n",
    "# Creamos un tensor de ejemplo con valores aleatorios\n",
    "x = torch.randn(batch_size, embedding_dim, L)\n",
    "features = [4, 8]\n",
    "for f in range(6):\n",
    "    features.append(8)\n",
    "\n",
    "    for mode in [\"upsample\", \"transpose\"]:\n",
    "        for add in [\"sum\", \"cat\"]:\n",
    "            for skip in [True, False]:\n",
    "                print(f\"m {mode} a {add} s {skip}\")\n",
    "                model = SimpleUNet(\n",
    "                    embedding_dim=4,\n",
    "                    num_conv=2,\n",
    "                    features=[4, 8, 8],\n",
    "                    up_mode=mode,\n",
    "                    addition=add,\n",
    "                    skip=skip,\n",
    "                )\n",
    "                output = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEQ2SEQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_len=0,\n",
    "        embedding_dim=4,\n",
    "        device=\"cpu\",\n",
    "        lr=1e-3,\n",
    "        scheduler=\"none\",\n",
    "        output_th=0.5,\n",
    "        verbose=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Base instantiation of model\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.verbose = verbose\n",
    "        self.config = kwargs\n",
    "        self.output_th = output_th\n",
    "\n",
    "        self.hyperparameters = {\n",
    "            \"hyp_device\": device,\n",
    "            \"hyp_lr\": lr,\n",
    "            \"hyp_scheduler\": scheduler,\n",
    "            \"hyp_verbose\": verbose,\n",
    "            \"hyp_output_th\": output_th,\n",
    "        }\n",
    "        # Define architecture\n",
    "        self.build_graph(embedding_dim, **kwargs)\n",
    "        self.optimizer = tr.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def build_graph(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        num_conv=2,\n",
    "        up_mode=\"transpose\",\n",
    "        skip=False,\n",
    "        addition=\"cat\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        self.features = [4, 8, 8, 8]\n",
    "        self.r_features = self.features[::-1]\n",
    "        self.encoder_blocks = len(self.features) - 1\n",
    "        self.L_min = 128 // ((2**self.encoder_blocks))\n",
    "        volume = [(128 / 2**i) * f for i, f in enumerate(self.features)]\n",
    "\n",
    "        self.architecture = {\n",
    "            \"arc_embedding_dim\": embedding_dim,\n",
    "            \"arc_encoder_blocks\": self.encoder_blocks,\n",
    "            \"arc_initial_volume\": embedding_dim * 128,\n",
    "            \"arc_latent_volume\": volume[-1],\n",
    "            \"arc_features\": self.features,\n",
    "            \"arc_num_conv\": num_conv,\n",
    "            \"arc_up_mode\": up_mode,\n",
    "            \"arc_addition\": addition,\n",
    "            \"arc_skip\": skip,\n",
    "        }\n",
    "        self.inc = N_Conv(embedding_dim, self.features[0], num_conv)\n",
    "\n",
    "        self.down = nn.ModuleList(\n",
    "            [\n",
    "                Max_Down(self.features[i], self.features[i + 1], num_conv)\n",
    "                for i in range(self.encoder_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.up = nn.ModuleList(\n",
    "            [\n",
    "                UpBlock(\n",
    "                    in_channels=self.r_features[i],\n",
    "                    out_channels=self.r_features[i + 1],\n",
    "                    num_conv=num_conv,\n",
    "                    up_mode=up_mode,\n",
    "                    skip=skip,\n",
    "                    addition=addition,\n",
    "                )\n",
    "                for i in range(len(self.r_features) - 1)\n",
    "            ]\n",
    "        )\n",
    "        self.outc = OutConv(self.features[0], embedding_dim)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # x = batch[\"embedding\"].to(self.device)\n",
    "        x = batch\n",
    "        print(\"Input shape:\", x.shape)\n",
    "        x = self.inc(x)\n",
    "        encoder_outputs = [x]\n",
    "        print(\"Encoder 0 shape:\", encoder_outputs[0].shape)\n",
    "        for i, down in enumerate(self.down):\n",
    "            x = down(x)\n",
    "            encoder_outputs.append(x)\n",
    "            print(f\"Encoder {i+1} shape:\", encoder_outputs[i + 1].shape)\n",
    "\n",
    "        x_latent = x\n",
    "        print(f\"X latent shape:\", x_latent.shape)\n",
    "\n",
    "        skips = encoder_outputs[:-1][::-1]\n",
    "        for up, skip in zip(self.up, skips):\n",
    "            print(f\"Up shape: {x.shape};  skip shape: {skip.shape}\")\n",
    "            x = up(x, skip)\n",
    "\n",
    "        x_rec = self.outc(x)\n",
    "\n",
    "        return x_rec, x_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 4, 128])\n",
      "Encoder 0 shape: torch.Size([4, 4, 128])\n",
      "Encoder 1 shape: torch.Size([4, 8, 64])\n",
      "Encoder 2 shape: torch.Size([4, 8, 32])\n",
      "Encoder 3 shape: torch.Size([4, 8, 16])\n",
      "X latent shape: torch.Size([4, 8, 16])\n",
      "Up shape: torch.Size([4, 8, 16]);  skip shape: torch.Size([4, 8, 32])\n",
      "Up shape: torch.Size([4, 8, 32]);  skip shape: torch.Size([4, 8, 64])\n",
      "Up shape: torch.Size([4, 8, 64]);  skip shape: torch.Size([4, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Seq2Seq(\n",
    "    train_len=0,\n",
    "    embedding_dim=4,\n",
    "    device=\"cpu\",\n",
    "    lr=1e-3,\n",
    "    scheduler=\"none\",\n",
    "    output_th=0.5,\n",
    "    verbose=True,\n",
    "    kernel=3,\n",
    "    num_conv=2,\n",
    "    up_mode=\"transpose\",\n",
    "    addition=\"cat\",\n",
    "    skip=False,\n",
    ")\n",
    "output = model(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
