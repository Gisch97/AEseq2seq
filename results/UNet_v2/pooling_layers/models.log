# Starting experiment at jue 30 ene 2025 02:32:34 -03
# Saving hyperparameters: NUM_CONV1 = 1 2 3 NUM_CONV2 = 1 2 3 
# Model file content:
import pandas as pd
import math
from dataclasses import dataclass
from torch import nn
from torchinfo import summary
from torch.nn.functional import mse_loss, cross_entropy
import torch as tr
from tqdm import tqdm
import mlflow
import mlflow.pytorch
from .metrics import compute_metrics
from .utils import mat2bp, postprocessing
from ._version import __version__


    
def seq2seq(weights=None, **kwargs): 
    """ 
    seq2seq: a deep learning-based autoencoder for RNA sequence to sequence prediction.
    weights (str): Path to weights file
    **kwargs: Model hyperparameters
    """
    
    model = Seq2Seq(**kwargs)
    if weights is not None:
        print(f"Load weights from {weights}")
        model.load_state_dict(tr.load(weights, map_location=tr.device(model.device)))
    else:
        print("No weights provided, using random initialization")
    model.log_model()
    mlflow.set_tag("model", 'Unet-avg-pooled')
    mlflow.set_tag("version", '2')
    return model
    
    
class Seq2Seq(nn.Module):
    def __init__(self,
        train_len=0,
        embedding_dim=4,
        device="cpu", 
        lr=1e-3,
        scheduler="none",
        output_th=0.5,
        verbose=True,
        **kwargs):
        """Base instantiation of model"""
        super().__init__()


        self.device = device
        self.verbose = verbose
        self.config = kwargs
        self.output_th = output_th
        
        self.hyperparameters = {
            "hyp_device": device, 
            "hyp_lr": lr,
            "hyp_scheduler": scheduler,
            "hyp_verbose": verbose, 
            "hyp_output_th": output_th
            }        
        # Define architecture
        self.build_graph(embedding_dim, **kwargs) 
        self.optimizer = tr.optim.Adam(self.parameters(), lr=lr)

        # lr scheduler
        self.scheduler_name = scheduler
        if scheduler == "plateau":
            self.scheduler = tr.optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, mode="max", patience=5, verbose=True
            )
        elif scheduler == "cycle":
            self.scheduler = tr.optim.lr_scheduler.OneCycleLR(
                self.optimizer, max_lr=lr, steps_per_epoch=train_len, epochs=self.config["max_epochs"]
            )
        else:
            self.scheduler = None

        self.to(device)
    
    def build_graph(
        self,
        embedding_dim,
        filters=4,
        kernel=3,
        num_layers=2,
        dilation_resnet1d=3,
        resnet_bottleneck_factor=0.5,
        rank=8,
        stride_1=1,
        stride_2=1,
        num_conv1=3,
        num_conv2=3,
        pool_stride_1=2,
        pool_stride_2=2,
        pool_kernel_1=2,
        pool_kernel_2=2,
        **kwargs
    ):         
        
        # Bloque 1 (encode1): 
        self.encode1_in = embedding_dim
        self.encode1_out = (2 ** num_conv1) * self.encode1_in 

        # Bloque 2 (encode2):  
        self.encode2_in = self.encode1_out
        self.encode2_out = (2 ** num_conv2) * self.encode2_in  

        # decode1 (invertir el bloque 2)
        self.decode1_in = self.encode2_out  
        self.decode2_in = self.encode1_out  

        self.architecture = {
            "arc_embedding_dim": embedding_dim,
            "arc_filters": self.encode1_in,
            "arc_rank": self.encode2_in,
            "arc_latent_dim": self.encode2_out,
            "arc_kernel": kernel,
            "arc_num_layers": num_layers,
            "arc_dilation_resnet1d": dilation_resnet1d,
            "arc_resnet_bottleneck_factor": resnet_bottleneck_factor,
            "arc_stride_1": stride_1,
            "arc_stride_2": stride_2,
            "arc_num_conv1": num_conv1,
            "arc_num_conv2": num_conv2,
            "arc_pool_stride_1": pool_stride_1,
            "arc_pool_stride_2": pool_stride_2,
            "arc_pool_kernel_1": pool_kernel_1,
            "arc_pool_kernel_2": pool_kernel_2
        }
        pad = (kernel - 1) // 2

        self.encode1 = conv_sequence_avg_pooled(
            input_channels=self.encode1_in, 
            num_conv=num_conv1, 
            padding=pad, 
            pool_stride=pool_stride_1,
            pool_kernel=pool_kernel_1
        )

        self.encode2 = nn.Sequential(
            *[ResidualLayer1D(
                dilation_resnet1d,
                resnet_bottleneck_factor,
                self.encode1_out,
                kernel
            ) for _ in range(num_layers)],
            conv_sequence_avg_pooled(
                input_channels=self.encode1_out,  
                num_conv=num_conv2, 
                padding=pad, 
                pool_stride=pool_stride_2,
                pool_kernel=pool_kernel_2 
            )
        )

        self.decode1 = transpose_conv_sequence(
                input_channels=self.decode1_in,  
                num_conv=num_conv2, 
                up_kernel=pool_kernel_2, 
                padding=pad, 
                up_stride=pool_stride_2
        )

        self.decode2 = nn.Sequential(
            *[ResidualLayer1D(
                dilation_resnet1d,
                resnet_bottleneck_factor,
                self.decode2_in,
                kernel
            ) for _ in range(num_layers)],
            transpose_conv_sequence(
                input_channels=self.decode2_in,  
                num_conv=num_conv1, 
                up_kernel=pool_kernel_1, 
                padding=pad, 
                up_stride=pool_stride_1
            )
        )
        
        # Bottleneck
        self.bottleneck = nn.Sequential(
            nn.Conv1d(self.encode2_out, self.encode2_out, kernel_size=kernel, padding=pad),
            nn.BatchNorm1d(self.encode2_out),
            nn.ReLU()
        )
    def forward(self, batch): 
        
        x = batch["embedding"].to(self.device) # (B, 4, 128)
        # print('x', x.shape)
        x1 = self.encode1(x)
        # print('x1', x1.shape) # (B, 16, 32)
        x2 = self.encode2(x1) 
        # print('x2', x2.shape) # (B, 64, 8)
        z = self.bottleneck(x2)
        # print('z', z.shape) # (B, 64, 8)
        x3 = self.decode1(z )       # (B, 16, 32)
        # print('x3', x3.shape)
        x_rec = self.decode2(x3)    # (B, 4, 128)
        # print('x_rec', x_rec.shape)
        return x_rec, z
 
    def loss_func(self, x_rec, x):
        """yhat and y are [N, L]"""
        x = x.view(x.shape[0], -1)
        x_rec = x_rec.view(x_rec.shape[0], -1)
        recon_loss = mse_loss(x_rec, x) 
        return recon_loss   

    
    def ce_loss_func(self, x_rec, x):
        """yhat and y are [N, L]"""
        x = x.view(x.shape[0], -1)
        x_rec = x_rec.view(x_rec.shape[0], -1)
        loss = cross_entropy(x_rec, x)
        return loss


    def fit(self, loader):
        self.train()

        metrics = {
            "loss": 0,
            "ce_loss": 0,
            "F1": 0,
            "Accuracy": 0,
            "Accuracy_seq": 0
            }
        if self.verbose: loader = tqdm(loader)

        for batch in loader: 
            x = batch["embedding"].to(self.device)
            # batch.pop("embedding")
            self.optimizer.zero_grad()  # Cleaning cache optimizer
            x_rec, z = self(batch)
            loss = self.loss_func(x_rec, x) 
            ce_loss = self.ce_loss_func(x_rec, x)
            metrics["loss"] += loss.item()
            metrics["ce_loss"] += ce_loss.item()
            
            
            batch_metrics = compute_metrics(x_rec, x, output_th=self.output_th)
            for k, v in batch_metrics.items():
                metrics[k] += v
            
            
            loss.backward()
            self.optimizer.step()

            if self.scheduler_name == "cycle":
                    self.scheduler.step()

        for k in metrics:
            metrics[k] /= len(loader)

        return metrics

    def test(self, loader):
        self.eval()
        
        metrics = {
            "loss": 0,
            "ce_loss": 0,
            "F1": 0,
            "Accuracy": 0,
            "Accuracy_seq": 0
            }

        if self.verbose:
            loader = tqdm(loader)

        with tr.no_grad():
            for batch in loader:  
                x = batch["embedding"].to(self.device)
                # batch.pop("embedding")
                lengths = batch["length"]
                
                x_rec, z = self(batch)
                loss = self.loss_func(x_rec, x)
                ce_loss = self.ce_loss_func(x_rec, x)
                metrics["loss"] += loss.item()
                metrics["ce_loss"] += ce_loss.item()
                
                
                batch_metrics = compute_metrics(x_rec, x, output_th=self.output_th)
                for k, v in batch_metrics.items():
                    metrics[k] += v

        for k in metrics: metrics[k] /= len(loader)

        return metrics

    def pred(self, loader, logits=False):
        self.eval()

        if self.verbose:
            loader = tqdm(loader)

        predictions, logits_list = [], [] 
        with tr.no_grad():
            for batch in loader: 
                
                seqid = batch["id"]
                embedding = batch["embedding"]
                sequences = batch["sequence"]
                lengths = batch["length"]
                x_rec, z = self(batch)
                
                for k in range(x_rec.shape[0]):
                    seq_len = lengths[k]
                
                    predictions.append((
                        seqid[k],
                        sequences[k],
                        seq_len,
                        embedding[k, :, :seq_len].cpu().numpy(),
                        x_rec[k, :, :seq_len].cpu().numpy(),
                        z[k].cpu().numpy()
                    ))
                    
        predictions = pd.DataFrame(predictions, columns=["id", "sequence", "length", "embedding", "reconstructed", "latent"])

        return predictions, logits_list

    def log_model(self):
        """Logs the model architecture and hyperparameters to MLflow.""" 
        mlflow.log_params(self.hyperparameters)
        mlflow.log_params(self.architecture)

class ResidualLayer1D(nn.Module):
    def __init__(
        self,
        dilation,
        resnet_bottleneck_factor,
        filters,
        kernel_size,
    ):
        super().__init__()

        num_bottleneck_units = math.floor(resnet_bottleneck_factor * filters)

        self.layer = nn.Sequential(
            nn.BatchNorm1d(filters),
            nn.ReLU(),
            nn.Conv1d(
                filters,
                num_bottleneck_units,
                kernel_size,
                dilation=dilation,
                padding="same",
            ),
            nn.BatchNorm1d(num_bottleneck_units),
            nn.ReLU(),
            nn.Conv1d(num_bottleneck_units, filters, kernel_size=1, padding="same"),
        )

    def forward(self, x):
        return x + self.layer(x)
    

def conv_sequence_avg_pooled(input_channels, num_conv=1,  padding=1, pool_stride=2, pool_kernel=2):
    # Every conv: out_channels = input_channels * growth_factor
    # It accumulates for the next layer.
    
    layers = []
    current_channels = input_channels
    
    for _ in range(num_conv):
        next_channels = int(current_channels * pool_stride)
        layers.append(nn.Conv1d(current_channels, next_channels, kernel_size=3, padding=padding, stride=1))
        layers.append(nn.ReLU(inplace=True))
        layers.append(nn.AvgPool1d(kernel_size=pool_kernel, stride=pool_stride, padding=0))
        current_channels = next_channels
    return nn.Sequential(*layers)

def transpose_conv_sequence(input_channels, growth_factor=2, num_conv=1, up_kernel=2, padding=1, up_stride=2):
    layers = []
    current_channels = input_channels
    
    for _ in range(num_conv):
        next_channels = int(current_channels // growth_factor)
        layers.append(nn.ConvTranspose1d(current_channels, next_channels, kernel_size=up_kernel, stride=up_stride, padding=0))
        layers.append(nn.ReLU(inplace=True))
        
        current_channels = next_channels
        layers.append(nn.Conv1d(current_channels, current_channels, kernel_size=3, padding=padding, stride=1))    
    return nn.Sequential(*layers)